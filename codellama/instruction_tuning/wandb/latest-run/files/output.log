  0%|                                                                                                                     | 0/400 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  2%|██▋                                                                                                       | 10/400 [10:29<5:32:24, 51.14s/it]










  5%|█████▎                                                                                                    | 20/400 [16:37<3:37:32, 34.35s/it]
{'loss': 1.083, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.02}















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1524/1525 [20:50<00:00,  1.30it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  8%|███████▉                                                                                                  | 30/400 [42:03<4:13:09, 41.05s/it]









 10%|██████████▎                                                                                               | 39/400 [45:15<2:06:09, 20.97s/it]
 10%|██████████▌                                                                                               | 40/400 [45:34<2:01:37, 20.27s/it]














































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1524/1525 [20:46<00:00,  1.33it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 12%|█████████████                                                                                           | 50/400 [1:08:54<2:48:19, 28.86s/it]









 15%|███████████████▎                                                                                        | 59/400 [1:18:26<4:59:48, 52.75s/it]
 15%|███████████████▌                                                                                        | 60/400 [1:19:10<4:44:41, 50.24s/it]
















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.5209912657737732, 'eval_runtime': 1253.4222, 'eval_samples_per_second': 9.73, 'eval_steps_per_second': 1.217, 'epoch': 0.07}









 18%|██████████████████▏                                                                                     | 70/400 [1:46:11<4:32:03, 49.47s/it]









 20%|████████████████████▌                                                                                   | 79/400 [1:50:21<2:25:43, 27.24s/it]
 20%|████████████████████▊                                                                                   | 80/400 [1:50:45<2:20:35, 26.36s/it]

















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:52<00:01,  1.25it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 22%|███████████████████████▏                                                                                | 89/400 [2:14:54<3:38:26, 42.14s/it]











 25%|█████████████████████████▊                                                                             | 100/400 [2:17:43<1:10:59, 14.20s/it]
{'loss': 0.4753, 'learning_rate': 0.000294, 'epoch': 0.12}
















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.4853839874267578, 'eval_runtime': 1253.2459, 'eval_samples_per_second': 9.732, 'eval_steps_per_second': 1.217, 'epoch': 0.12}









 28%|████████████████████████████▎                                                                          | 110/400 [2:48:50<5:15:23, 65.25s/it]









 30%|██████████████████████████████▋                                                                        | 119/400 [2:54:27<2:49:13, 36.13s/it]
 30%|██████████████████████████████▉                                                                        | 120/400 [2:54:59<2:42:57, 34.92s/it]

















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1522/1525 [20:51<00:02,  1.39it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 32%|█████████████████████████████████▏                                                                     | 129/400 [3:20:08<3:39:21, 48.57s/it]










 35%|███████████████████████████████████▊                                                                   | 139/400 [3:23:49<1:33:15, 21.44s/it]

 35%|████████████████████████████████████                                                                   | 140/400 [3:24:08<1:29:34, 20.67s/it]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:50<00:01,  1.25it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 37%|██████████████████████████████████████▎                                                                | 149/400 [3:47:22<2:32:23, 36.43s/it]










 40%|████████████████████████████████████████▉                                                              | 159/400 [3:57:02<3:31:42, 52.71s/it]
 40%|█████████████████████████████████████████▏                                                             | 160/400 [3:57:47<3:21:23, 50.35s/it]
















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.4571021795272827, 'eval_runtime': 1252.2298, 'eval_samples_per_second': 9.739, 'eval_steps_per_second': 1.218, 'epoch': 0.19}








 42%|███████████████████████████████████████████▌                                                           | 169/400 [4:24:12<3:37:33, 56.51s/it]











 45%|██████████████████████████████████████████████▎                                                        | 180/400 [4:29:14<1:35:57, 26.17s/it]
{'loss': 0.4571, 'learning_rate': 0.00022199999999999998, 'epoch': 0.21}
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:51<00:01,  1.25it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 47%|████████████████████████████████████████████████▋                                                      | 189/400 [4:53:21<2:28:05, 42.11s/it]










 50%|████████████████████████████████████████████████████▏                                                    | 199/400 [4:55:58<50:22, 15.04s/it]
 50%|████████████████████████████████████████████████████▌                                                    | 200/400 [4:56:10<47:35, 14.28s/it]

















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1522/1525 [20:49<00:02,  1.39it/s]
 50%|████████████████████████████████████████████████████▌                                                    | 200/400 [5:17:03<47:35, 14.28s/it]/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 52%|█████████████████████████████████████████████████████▊                                                 | 209/400 [5:26:41<3:57:42, 74.68s/it]










 55%|████████████████████████████████████████████████████████▍                                              | 219/400 [5:33:09<1:50:10, 36.52s/it]
 55%|████████████████████████████████████████████████████████▋                                              | 220/400 [5:33:41<1:45:23, 35.13s/it]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1522/1525 [20:50<00:02,  1.36it/s]

 55%|████████████████████████████████████████████████████████▋                                              | 220/400 [5:54:35<1:45:23, 35.13s/it]/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 57%|██████████████████████████████████████████████████████████▉                                            | 229/400 [5:58:48<2:18:16, 48.52s/it]










 60%|██████████████████████████████████████████████████████████████▋                                          | 239/400 [6:02:27<56:44, 21.15s/it]

 60%|███████████████████████████████████████████████████████████████                                          | 240/400 [6:02:45<54:18, 20.36s/it]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:51<00:01,  1.20it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 62%|████████████████████████████████████████████████████████████████▍                                      | 250/400 [6:26:13<1:13:10, 29.27s/it]










 65%|██████████████████████████████████████████████████████████████████▉                                    | 260/400 [6:36:26<1:57:27, 50.34s/it]
{'loss': 0.4419, 'learning_rate': 0.00014199999999999998, 'epoch': 0.3}
















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.45137766003608704, 'eval_runtime': 1253.5969, 'eval_samples_per_second': 9.729, 'eval_steps_per_second': 1.216, 'epoch': 0.3}









 68%|█████████████████████████████████████████████████████████████████████▌                                 | 270/400 [7:03:30<1:47:29, 49.61s/it]









 70%|█████████████████████████████████████████████████████████████████████████▏                               | 279/400 [7:07:39<54:38, 27.10s/it]
 70%|█████████████████████████████████████████████████████████████████████████▌                               | 280/400 [7:08:04<52:36, 26.30s/it]

















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1522/1525 [20:51<00:02,  1.39it/s]
 70%|█████████████████████████████████████████████████████████████████████████▌                               | 280/400 [7:28:58<52:36, 26.30s/it]/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 72%|██████████████████████████████████████████████████████████████████████████▋                            | 290/400 [7:32:30<1:04:28, 35.17s/it]










 75%|██████████████████████████████████████████████████████████████████████████████▊                          | 300/400 [7:35:02<23:42, 14.22s/it]
{'loss': 0.4705, 'learning_rate': 0.000102, 'epoch': 0.35}















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.44770729541778564, 'eval_runtime': 1251.3683, 'eval_samples_per_second': 9.746, 'eval_steps_per_second': 1.219, 'epoch': 0.35}








 77%|███████████████████████████████████████████████████████████████████████████████▌                       | 309/400 [8:05:32<1:53:19, 74.72s/it]










 80%|███████████████████████████████████████████████████████████████████████████████████▋                     | 319/400 [8:12:00<49:26, 36.63s/it]
 80%|████████████████████████████████████████████████████████████████████████████████████                     | 320/400 [8:12:32<47:01, 35.26s/it]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1522/1525 [20:50<00:02,  1.39it/s]

 80%|████████████████████████████████████████████████████████████████████████████████████                     | 320/400 [8:33:25<47:01, 35.26s/it]/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 82%|██████████████████████████████████████████████████████████████████████████████████████▎                  | 329/400 [8:37:37<57:13, 48.36s/it]










 85%|████████████████████████████████████████████████████████████████████████████████████████▉                | 339/400 [8:41:15<21:35, 21.23s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████████████▎               | 340/400 [8:41:34<20:33, 20.55s/it]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 1521/1525 [20:48<00:02,  1.42it/s]
 85%|█████████████████████████████████████████████████████████████████████████████████████████▎               | 340/400 [9:02:26<20:33, 20.55s/it]/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 88%|███████████████████████████████████████████████████████████████████████████████████████████▉             | 350/400 [9:05:00<24:16, 29.13s/it]










 90%|██████████████████████████████████████████████████████████████████████████████████████████████▌          | 360/400 [9:15:09<33:03, 49.58s/it]
{'loss': 0.4296, 'learning_rate': 4.2e-05, 'epoch': 0.42}















































































































































































































































































































































































































































































































































































































































  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.4442940950393677, 'eval_runtime': 1251.2031, 'eval_samples_per_second': 9.747, 'eval_steps_per_second': 1.219, 'epoch': 0.42}









 92%|█████████████████████████████████████████████████████████████████████████████████████████████████▏       | 370/400 [9:42:07<24:41, 49.37s/it]









 95%|███████████████████████████████████████████████████████████████████████████████████████████████████▊     | 380/400 [9:46:41<08:47, 26.38s/it]
  0%|▏                                                                                                           | 2/1525 [00:00<06:03,  4.19it/s]
















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:50<00:01,  1.25it/s]
  warnings.warn(
/shared/data3/xzhong23/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 389/400 [10:10:48<07:46, 42.37s/it]










100%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋| 399/400 [10:13:29<00:15, 15.46s/it]

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [10:13:42<00:00, 14.53s/it]



















































































































































































































































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1523/1525 [20:55<00:01,  1.25it/s]
{'eval_loss': 0.4415822923183441, 'eval_runtime': 1257.9718, 'eval_samples_per_second': 9.695, 'eval_steps_per_second': 1.212, 'epoch': 0.47}
{'train_runtime': 38087.2283, 'train_samples_per_second': 1.344, 'train_steps_per_second': 0.011, 'train_loss': 0.5101738035678863, 'epoch': 0.47}
Finish training